{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://habr.com/ru/search'\n",
    "KEYWORDS = ['python', 'парсинг']\n",
    "\n",
    "def get_all_links(url, query, pages):\n",
    "    all_refs = []\n",
    "    params = {\n",
    "        'q': query\n",
    "    }\n",
    "    for i in range(pages):\n",
    "        params['page'] = i + 1\n",
    "        res = requests.get(URL, params)\n",
    "        time.sleep(0.3)\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        news_blocks = soup.find_all('li', class_='content-list__item content-list__item_post shortcuts_item')\n",
    "        articles_intro = list(map(lambda x: x.find('div', class_='post__body post__body_crop'), news_blocks))\n",
    "        all_refs += list(map(lambda x: x.find('a').get('href'), articles_intro))\n",
    "    return all_refs\n",
    "\n",
    "all_links = get_all_links(URL, KEYWORDS, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-04-03</td>\n",
       "      <td>Правда про парсинг сайтов, или «все интернет-м...</td>\n",
       "      <td>https://habr.com/ru/post/446488/#habracut</td>\n",
       "      <td>\\nВ этой статье я постараюсь наиболее просто р...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-05-07</td>\n",
       "      <td>Парсинг сайтов — а это вообще легально в России?</td>\n",
       "      <td>https://habr.com/ru/post/450834/#habracut</td>\n",
       "      <td>\\nПо одному из определений парсинг есть синтак...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-10-01</td>\n",
       "      <td>Как выбрать решение для парсинга сайтов: класс...</td>\n",
       "      <td>https://habr.com/ru/post/521646/#habracut</td>\n",
       "      <td>\\nПарсинг или как его еще иногда называют web ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-10-17</td>\n",
       "      <td>Парсинг сайтов: как с точки зрения закона выгл...</td>\n",
       "      <td>https://habr.com/ru/post/340302/#habracut</td>\n",
       "      <td>\\n\\r\\nПопробуем рассмотреть один из лучших спо...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-12-01</td>\n",
       "      <td>Пример использования Product API от Fetchee дл...</td>\n",
       "      <td>https://habr.com/ru/company/fetchee/blog/31655...</td>\n",
       "      <td>\\n\\n\\r\\nВ этой инструкции мы расскажем о том, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-07-19</td>\n",
       "      <td>Использование morph.io для веб-парсинга</td>\n",
       "      <td>https://habr.com/ru/post/262991/#habracut</td>\n",
       "      <td>\\nЕсли вы читали предыдущие две статьи, Веб-па...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-02-14</td>\n",
       "      <td>Парсинг сайтов-магазинов. Личный опыт и немног...</td>\n",
       "      <td>https://habr.com/ru/post/169409/#habracut</td>\n",
       "      <td>\\nРазделим парсинг (скраппинг) сайтов на две п...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-05-25</td>\n",
       "      <td>Еще о парсинге на Prolog'е</td>\n",
       "      <td>https://habr.com/ru/post/60430/#habracut</td>\n",
       "      <td>\\nВот тут наткнулся на, в общем-то, простую за...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-09-02</td>\n",
       "      <td>Grab — python библиотека для парсинга сайтов</td>\n",
       "      <td>https://habr.com/ru/post/127584/#habracut</td>\n",
       "      <td>\\nЛет пять-шесть назад, когда я ещё программир...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-12-16</td>\n",
       "      <td>Парсинг формул в 50 строк на Python</td>\n",
       "      <td>https://habr.com/ru/post/273253/#habracut</td>\n",
       "      <td>\\nВдохновение — задача с собеседования Яндекса...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          date                                              title  \\\n",
       "0   2019-04-03  Правда про парсинг сайтов, или «все интернет-м...   \n",
       "0   2019-05-07   Парсинг сайтов — а это вообще легально в России?   \n",
       "0   2020-10-01  Как выбрать решение для парсинга сайтов: класс...   \n",
       "0   2017-10-17  Парсинг сайтов: как с точки зрения закона выгл...   \n",
       "0   2016-12-01  Пример использования Product API от Fetchee дл...   \n",
       "..         ...                                                ...   \n",
       "0   2015-07-19            Использование morph.io для веб-парсинга   \n",
       "0   2013-02-14  Парсинг сайтов-магазинов. Личный опыт и немног...   \n",
       "0   2009-05-25                         Еще о парсинге на Prolog'е   \n",
       "0   2011-09-02       Grab — python библиотека для парсинга сайтов   \n",
       "0   2015-12-16                Парсинг формул в 50 строк на Python   \n",
       "\n",
       "                                                 link  \\\n",
       "0           https://habr.com/ru/post/446488/#habracut   \n",
       "0           https://habr.com/ru/post/450834/#habracut   \n",
       "0           https://habr.com/ru/post/521646/#habracut   \n",
       "0           https://habr.com/ru/post/340302/#habracut   \n",
       "0   https://habr.com/ru/company/fetchee/blog/31655...   \n",
       "..                                                ...   \n",
       "0           https://habr.com/ru/post/262991/#habracut   \n",
       "0           https://habr.com/ru/post/169409/#habracut   \n",
       "0            https://habr.com/ru/post/60430/#habracut   \n",
       "0           https://habr.com/ru/post/127584/#habracut   \n",
       "0           https://habr.com/ru/post/273253/#habracut   \n",
       "\n",
       "                                                 text  \n",
       "0   \\nВ этой статье я постараюсь наиболее просто р...  \n",
       "0   \\nПо одному из определений парсинг есть синтак...  \n",
       "0   \\nПарсинг или как его еще иногда называют web ...  \n",
       "0   \\n\\r\\nПопробуем рассмотреть один из лучших спо...  \n",
       "0   \\n\\n\\r\\nВ этой инструкции мы расскажем о том, ...  \n",
       "..                                                ...  \n",
       "0   \\nЕсли вы читали предыдущие две статьи, Веб-па...  \n",
       "0   \\nРазделим парсинг (скраппинг) сайтов на две п...  \n",
       "0   \\nВот тут наткнулся на, в общем-то, простую за...  \n",
       "0   \\nЛет пять-шесть назад, когда я ещё программир...  \n",
       "0   \\nВдохновение — задача с собеседования Яндекса...  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_kom_news(links):\n",
    "    kom_news = pd.DataFrame()\n",
    "    for link in all_links:\n",
    "        soup = BeautifulSoup(requests.get(link).text, 'html.parser')\n",
    "        if soup.find('span', class_='post__time'):\n",
    "            date = pd.to_datetime(soup.find('span', class_='post__time').get('data-time_published'), dayfirst=True).date()\n",
    "        elif soup.find('time', class_='title__cake'):\n",
    "            date = pd.to_datetime(soup.find('time', class_='title__cake').get('datetime'), dayfirst=True).date()\n",
    "        if soup.find('span', class_='post__title-text'): \n",
    "            title = soup.find('span', class_='post__title-text').text\n",
    "        else: \n",
    "            title = soup.find('span', class_='article_name').text    \n",
    "        text = soup.find('div', class_='post__body post__body_full').text\n",
    "        row = {'date': date, 'title': title, 'link': link,'text': text}\n",
    "        kom_news = pd.concat([kom_news, pd.DataFrame([row])])  \n",
    "    return kom_news\n",
    "kom_news = get_kom_news(all_links)\n",
    "kom_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-f85034da6cf5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m Подсказка: сервис работает при помощи \"скрытого\" API. Внимательно изучите post-запросы.\"\"\"\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"Написать скрипт, который будет проверять список e-mail адресов на утечку при помощи сервиса Avast Hack Ckeck.\n",
    "Список email-ов задаем переменной в начале кода:\n",
    "EMAIL = [xxx@x.ru, yyy@y.com]\n",
    "В итоге должен формироваться датафрейм со столбцами: <почта> - <дата утечки> - <источник утечки> - <описание утечки>\n",
    "Подсказка: сервис работает при помощи \"скрытого\" API. Внимательно изучите post-запросы.\"\"\"\n",
    "\n",
    "Совершенно непонятно, как делать. Ничего не знаю про HTML, целый день убит просто на попытки подобрать правильные \"теги\"\n",
    "в первом задании. Здесь вообще непонятно как подступиться, где этот скрытый API, куда смотреть... тут даже ноутбук к\n",
    "уроку не помог. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
